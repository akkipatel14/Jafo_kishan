{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vHpXXy5Ss_dl"
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# EDA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Data preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# utilities\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sgjw379-0FxM",
    "outputId": "b29ed0ff-7cca-4314-f9ef-2c7ca58bb0b8"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Akshay /plant\\\\sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m checkpoints_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.ipynb_checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Read the CSV files\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m sample_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_data_path)\n\u001b[0;32m     17\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_data_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Akshay /plant\\\\sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "directory_path = '/content/drive/MyDrive/Akshay /plant'\n",
    "\n",
    "# Construct file paths based on the directory path\n",
    "sample_data_path = os.path.join(directory_path, 'sample_submission.csv')\n",
    "train_data_path = os.path.join(directory_path, 'train.csv')\n",
    "test_data_path = os.path.join(directory_path, 'test.csv')\n",
    "model_path = os.path.join(directory_path, 'paddy_disease_model.h5')\n",
    "notebook_copy1_path = os.path.join(directory_path, 'Untitled6-Copy1.ipynb')\n",
    "notebook_path = os.path.join(directory_path, 'Untitled6.ipynb')\n",
    "images_dir = os.path.join(directory_path, 'images')\n",
    "checkpoints_dir = os.path.join(directory_path, '.ipynb_checkpoints')\n",
    "\n",
    "# Read the CSV files\n",
    "sample_data = pd.read_csv(sample_data_path)\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Set the image directory\n",
    "image_dir = images_dir + '/'\n",
    "\n",
    "# Output the constructed file paths and directory paths\n",
    "print(\"Sample Data Path:\", sample_data_path)\n",
    "print(\"Train Data Path:\", train_data_path)\n",
    "print(\"Test Data Path:\", test_data_path)\n",
    "print(\"Model Path:\", model_path)\n",
    "print(\"Notebook Copy1 Path:\", notebook_copy1_path)\n",
    "print(\"Notebook Path:\", notebook_path)\n",
    "print(\"Images Directory:\", images_dir)\n",
    "print(\"Checkpoints Directory:\", checkpoints_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz-MJsbowXn6"
   },
   "outputs": [],
   "source": [
    "def print_short_summary(name, data):\n",
    "    \"\"\"\n",
    "    Print data head, shape and info.\n",
    "\n",
    "    Args:\n",
    "        name (str): name of dataset\n",
    "        data (dataframe): dataset in a pd.DataFrame format\n",
    "    \"\"\"\n",
    "    print(name)\n",
    "    print('\\n1. Data head:')\n",
    "    print(data.head())\n",
    "    print('\\n2. Data shape: {}'.format(data.shape))\n",
    "    print('\\n3. Data info:')\n",
    "    data.info()\n",
    "\n",
    "def print_number_files(dirpath):\n",
    "    print('{}: {} files'.format(dirpath, len(os.listdir(dirpath))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50XmxaI5wawK",
    "outputId": "f06cbd13-d33b-418d-b342-17b0484ed6ac"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Train data', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Auc_t313wfPc",
    "outputId": "82776f3f-999b-4238-e2f6-4010954b8fc4"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Test data', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n-1LLvycwiZC",
    "outputId": "baf18c4b-a90d-4be5-96d8-c056e7518aaa"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Sample data', sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiJuCXRHwl8K",
    "outputId": "84b4eeb0-59f2-41b7-f912-85c80658760b"
   },
   "outputs": [],
   "source": [
    "print_number_files('/content/drive/MyDrive/Akshay /plant/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbK-VGYgxjKT"
   },
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "del print_short_summary, print_number_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "DIHXKDefxmSr",
    "outputId": "6e3f39ad-0065-4c79-f85d-b2b374b8a837"
   },
   "outputs": [],
   "source": [
    "# Plot horizontal barplot of number of records per label\n",
    "plt.figure(figsize=(16, 9))\n",
    "tmp = train_data.iloc[:, 1:].sum().values\n",
    "tmp = pd.DataFrame(tmp, train_data.columns[1:]).sort_values(by=[0], ascending = False)\n",
    "sns.barplot(y = tmp.index, x = np.ravel(tmp.values), orient='h')\n",
    "plt.xlabel('Number of records')\n",
    "plt.ylabel('Label')\n",
    "plt.title('Number of records per label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zr4ip9sAxoS6"
   },
   "outputs": [],
   "source": [
    "def get_images_to_plot(file_names):\n",
    "    \"\"\"\n",
    "    Return list of image objects.\n",
    "\n",
    "    Args:\n",
    "        file_names: list of filenames\n",
    "    Returns:\n",
    "        list: list of image objects\n",
    "    \"\"\"\n",
    "    return [Image.open(f) for f in file_names]\n",
    "\n",
    "def get_image_label(dirname, data, labels, n = 5):\n",
    "    \"\"\"\n",
    "    Return dictionary with label-imagepath.\n",
    "\n",
    "    Args:\n",
    "        dirname: name of the directory\n",
    "        data: dataset of file names\n",
    "        labels: list of labels\n",
    "        n (opt): number of images per label\n",
    "    Returns:\n",
    "        dict: dictionary with label-imagepath pairs\n",
    "    \"\"\"\n",
    "    dict_img = {}\n",
    "    for l in labels:\n",
    "        indexes = data[l] == 1\n",
    "        tmp = data[indexes][:n]\n",
    "        tmp = dirname + tmp['image_id'] + '.jpg'\n",
    "        tmp = tmp.values\n",
    "        tmp = get_images_to_plot(tmp)\n",
    "        dict_img[l] = tmp\n",
    "\n",
    "    return dict_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yMJiUHpxyur",
    "outputId": "6215ff86-a990-4a63-d4f0-b3727ba8f9e9"
   },
   "outputs": [],
   "source": [
    "# Print original image size\n",
    "img_path = image_dir + train_data['image_id'][0] + '.jpg'\n",
    "img = Image.open(img_path)\n",
    "print('Original image size: {}'.format(img.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmXJ4LlN0zM1"
   },
   "outputs": [],
   "source": [
    "def print_short_summary(name, data):\n",
    "    \"\"\"\n",
    "    Print data head, shape and info.\n",
    "\n",
    "    Args:\n",
    "        name (str): name of dataset\n",
    "        data (dataframe): dataset in a pd.DataFrame format\n",
    "    \"\"\"\n",
    "    print(name)\n",
    "    print('\\n1. Data head:')\n",
    "    print(data.head())\n",
    "    print('\\n2. Data shape: {}'.format(data.shape))\n",
    "    print('\\n3. Data info:')\n",
    "    data.info()\n",
    "\n",
    "def print_number_files(dirpath):\n",
    "    print('{}: {} files'.format(dirpath, len(os.listdir(dirpath))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ly7ad7wX02Ia",
    "outputId": "d50f20de-6c66-4a29-dc7d-85679aeed059"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Train data', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIRxHCd604NV",
    "outputId": "593e447b-ba76-45a5-e46c-406c054749ef"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Test data', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bT9Jx03o08wr",
    "outputId": "39ac413e-1db1-48bb-8d6d-f98dd7ad4433"
   },
   "outputs": [],
   "source": [
    "print_short_summary('Sample data', sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xteD0sd1AML",
    "outputId": "f0c04d6d-6e20-41c0-d12f-c72a94158d09"
   },
   "outputs": [],
   "source": [
    "print_number_files(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_H7Z-vm_1AS8"
   },
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "del print_short_summary, print_number_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "4VwfRVjE1Hb8",
    "outputId": "d8217071-3a6f-46ac-dbc7-4eb35a6a3569"
   },
   "outputs": [],
   "source": [
    "# Plot horizontal barplot of number of records per label\n",
    "plt.figure(figsize=(16, 9))\n",
    "tmp = train_data.iloc[:, 1:].sum().values\n",
    "tmp = pd.DataFrame(tmp, train_data.columns[1:]).sort_values(by=[0], ascending = False)\n",
    "sns.barplot(y = tmp.index, x = np.ravel(tmp.values), orient='h')\n",
    "plt.xlabel('Number of records')\n",
    "plt.ylabel('Label')\n",
    "plt.title('Number of records per label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hESheKTK1Ojl"
   },
   "outputs": [],
   "source": [
    "def get_images_to_plot(file_names):\n",
    "    \"\"\"\n",
    "    Return list of image objects.\n",
    "\n",
    "    Args:\n",
    "        file_names: list of filenames\n",
    "    Returns:\n",
    "        list: list of image objects\n",
    "    \"\"\"\n",
    "    return [Image.open(f) for f in file_names]\n",
    "\n",
    "def get_image_label(dirname, data, labels, n = 5):\n",
    "    \"\"\"\n",
    "    Return dictionary with label-imagepath.\n",
    "\n",
    "    Args:\n",
    "        dirname: name of the directory\n",
    "        data: dataset of file names\n",
    "        labels: list of labels\n",
    "        n (opt): number of images per label\n",
    "    Returns:\n",
    "        dict: dictionary with label-imagepath pairs\n",
    "    \"\"\"\n",
    "    dict_img = {}\n",
    "    for l in labels:\n",
    "        indexes = data[l] == 1\n",
    "        tmp = data[indexes][:n]\n",
    "        tmp = dirname + tmp['image_id'] + '.jpg'\n",
    "        tmp = tmp.values\n",
    "        tmp = get_images_to_plot(tmp)\n",
    "        dict_img[l] = tmp\n",
    "\n",
    "    return dict_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uJVnUZo1R57",
    "outputId": "d4db1cce-1946-46eb-eef2-53c4f61b3679"
   },
   "outputs": [],
   "source": [
    "# Print original image size\n",
    "img_path = image_dir + train_data['image_id'][0] + '.jpg'\n",
    "img = Image.open(img_path)\n",
    "print('Original image size: {}'.format(img.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKZNKwwb1U0M"
   },
   "outputs": [],
   "source": [
    "# Get 5 filenames per label\n",
    "data = get_image_label(image_dir, train_data, tmp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "Byd5V0hH1ZrF",
    "outputId": "9b51e226-c886-44fb-93ca-6d36fedb22a4"
   },
   "outputs": [],
   "source": [
    "# Initialize subplots with number of labels rows and 5 columns\n",
    "fig, axes = plt.subplots(nrows=len(tmp.index), ncols=5, figsize=(16, 9))\n",
    "\n",
    "# Loop through selected images and display in the respective rows\n",
    "labels = tmp.index\n",
    "for i in range(len(labels)*5):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    axes[row, col].imshow(data[labels[row]][col])\n",
    "    axes[row, col].set_title(labels[row])\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5tddp_b1ieL"
   },
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "del get_images_to_plot, get_image_label, img_path, img\n",
    "del data, fig, axes, labels, row, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwYFdRFV1oFD"
   },
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "NUM_EPOCHS = 15\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d4cB5qgk1q8N",
    "outputId": "90bd9b8c-575d-4990-8bbb-c594e6700eff"
   },
   "outputs": [],
   "source": [
    "# Get class distribution\n",
    "class_counts = train_data.iloc[:, 1:].sum()\n",
    "\n",
    "# Get maximum count of the majority class\n",
    "max_count = class_counts.max()\n",
    "\n",
    "# Upsample the minority classes to the max_count records\n",
    "balanced_data = []\n",
    "for class_name in class_counts.index:\n",
    "    class_data = train_data[train_data[class_name] == 1]\n",
    "    upsampled_data = class_data.sample(max_count\n",
    "                                         , replace = True\n",
    "                                         , random_state = 0)\n",
    "    balanced_data.append(upsampled_data)\n",
    "\n",
    "# Get final balanced dataframe\n",
    "train_data_balanced = pd.concat(balanced_data, axis=0, ignore_index=True)\n",
    "train_data_balanced.iloc[:, 1:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_HQWzPF1vfd"
   },
   "outputs": [],
   "source": [
    "def get_augmented_image(image):\n",
    "    \"\"\"\n",
    "    Return augmented image.\n",
    "\n",
    "    Args:\n",
    "        image: image tensor\n",
    "    Returns:\n",
    "        image\n",
    "    \"\"\"\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, max_delta=0.1)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4SZfd6k1yGb"
   },
   "outputs": [],
   "source": [
    "# Get image paths and labels\n",
    "image_paths = image_dir + train_data_balanced['image_id'] + '.jpg'\n",
    "image_paths = image_paths.values\n",
    "labels = train_data_balanced.loc[:, 'healthy':]\n",
    "\n",
    "# Split into train and test sets fo training accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_paths\n",
    "                                                    , labels\n",
    "                                                    , test_size = 0.25\n",
    "                                                    , shuffle = True\n",
    "                                                    , random_state = 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDM8RKF110q8"
   },
   "outputs": [],
   "source": [
    "del image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz6QmEeu16bD"
   },
   "outputs": [],
   "source": [
    "def get_decoded_image(image_path, label = None):\n",
    "    \"\"\"\n",
    "    Load and preprocess images using TensorFlow I/O\n",
    "    and Image Generator transformation.\n",
    "\n",
    "    Args:\n",
    "        image_path: path to JPEG image\n",
    "    Returns:\n",
    "        image or tuple: decoded image or (image, label)\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Set 3 channels, RGB\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image.set_shape([None, None, 3])\n",
    "    # Set size to convert to compatible with models input\n",
    "    image = tf.image.resize(image, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = get_augmented_image(image)\n",
    "\n",
    "    return image if label is None else (image, label)\n",
    "\n",
    "\n",
    "def get_prefetched_data(data):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset from image paths.\n",
    "    Execution in parallel.\n",
    "    Load, preprocess images and batch the data.\n",
    "    Prefetch batches to improve training performance.\n",
    "\n",
    "    Args:\n",
    "        data: ndarray of image paths + labels (opt)\n",
    "    Returns:\n",
    "        tf.data.Dataset: preprocessed and preloaded TensorFlow dataset for keras NN\n",
    "    \"\"\"\n",
    "    # Autotune the degree of parallelism during training\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # Create dataset from image paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "    # Apply parallel processing to load and preprocess images\n",
    "    dataset = dataset.map(get_decoded_image, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RuT9-cC2DO8"
   },
   "outputs": [],
   "source": [
    "# Get prefetched tf.data.DataSet for subsequent models\n",
    "train_dataset = get_prefetched_data((X_train, y_train))\n",
    "test_dataset = get_prefetched_data((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-HPhWwn2JZl"
   },
   "outputs": [],
   "source": [
    "def get_model(Model):\n",
    "    \"\"\"\n",
    "    Return Model architecture.\n",
    "\n",
    "    Args:\n",
    "        obj: model class\n",
    "    Returns:\n",
    "        obj: model architecture\n",
    "    \"\"\"\n",
    "    model = Model(weights='imagenet'\n",
    "                     , include_top=False\n",
    "                     , input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3))\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_resnet():\n",
    "    return get_model(ResNet50)\n",
    "\n",
    "def get_model_vggnet():\n",
    "    return get_model(VGG16)\n",
    "\n",
    "def get_model_inceptionnet():\n",
    "    return get_model(InceptionV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_fNmkuM2Mm2"
   },
   "outputs": [],
   "source": [
    "def get_compiled_model(func):\n",
    "    \"\"\"\n",
    "    Return model to be trained with a multi-GPU strategy.\n",
    "    Allows data parallelism by copying all of the model's variables\n",
    "    to each processor.\n",
    "\n",
    "    Args:\n",
    "        func: function to get model architecture\n",
    "    Returns:\n",
    "        compiled_model: tensorflow model\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # Create a MirroredStrategy.\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        print('No GPU available, falling back to CPU.')\n",
    "\n",
    "    with strategy.scope():\n",
    "        compiled_model = func()\n",
    "        # Add custom classification layers for our task\n",
    "        compiled_model = Sequential([\n",
    "            compiled_model\n",
    "            , layers.GlobalAveragePooling2D()\n",
    "            # Add dense layers with 128 and 64 units\n",
    "            , layers.Dense(128, activation=\"relu\")\n",
    "            , layers.Dense(64, activation=\"relu\")\n",
    "            # Add regularization with dropout rate 30%\n",
    "            , layers.Dropout(0.3)\n",
    "            , layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "        ])\n",
    "        compiled_model.compile(optimizer = 'adam'\n",
    "                              , loss = 'categorical_crossentropy'\n",
    "                              , metrics = ['categorical_accuracy'])\n",
    "\n",
    "    return compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkZpzJn52QEK"
   },
   "outputs": [],
   "source": [
    "def plot_model_scores(scores, model_name):\n",
    "    \"\"\"\n",
    "    Plot train and test accuracy scores of a model by epoch\n",
    "    \"\"\"\n",
    "    train_scores, test_scores = scores\n",
    "    epochs = range(1, len(train_scores) + 1)\n",
    "\n",
    "    # Plot train and test scores\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.plot(epochs, train_scores, label='Train score')\n",
    "    plt.plot(epochs, test_scores, label='Test score')\n",
    "    plt.title('Train and test accuracy scores of the {}'.format(model_name))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_model_results(model_name, model):\n",
    "    \"\"\"\n",
    "    Return tuple of runtime, train and test scores.\n",
    "    Compile, fit and save model along the way.\n",
    "\n",
    "    Args:\n",
    "        model: fitted model\n",
    "    Returns:\n",
    "        (runtime, (train_scores, test_scores) )\n",
    "    \"\"\"\n",
    "    model = get_compiled_model(model)\n",
    "\n",
    "    st = time.time()\n",
    "    model.fit(train_dataset, epochs = NUM_EPOCHS, validation_data=test_dataset)\n",
    "    runtime = time.time() - st\n",
    "\n",
    "    model.save('{}.h5'.format(model_name))\n",
    "\n",
    "    train_scores = model.history.history['categorical_accuracy']\n",
    "    test_scores = model.history.history['val_categorical_accuracy']\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    return (runtime, (train_scores, test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yfy_58YN2TW7",
    "outputId": "5dbe9321-ee9f-4aed-ab01-748a71f488e0"
   },
   "outputs": [],
   "source": [
    "# Get train and test scores of every epoch\n",
    "runtime_resnet, scores_resnet = get_model_results('model_resnet'\n",
    "                                                  ,get_model_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "id": "tMdwYsFqKBAg",
    "outputId": "c55a46f7-63c0-4a5f-ae2b-c90ef820c64b"
   },
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "plot_model_scores(scores_resnet, 'ResNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ibAXG3iecC_X",
    "outputId": "f10b22cf-232e-4a04-a9fb-de3cc65ebd80"
   },
   "outputs": [],
   "source": [
    "# Get train and test scores of every epoch\n",
    "runtime_vggnet, scores_vggnet = get_model_results('model_vggnet'\n",
    "                                                  ,get_model_vggnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661
    },
    "id": "jDxdDyumkYsm",
    "outputId": "445569d2-8b80-46de-e47a-8ee880992b9b"
   },
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "plot_model_scores(scores_vggnet, 'VGG-Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIVc51Tgkdst",
    "outputId": "c2ff9ac7-3ea1-4d91-b089-845b766cd5a3"
   },
   "outputs": [],
   "source": [
    "# Get train and test scores of every epoch\n",
    "runtime_inceptionnet, scores_inceptionnet = get_model_results('model_inceptionnet'\n",
    "                                                  ,get_model_inceptionnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "zVp0KdHJm3W-",
    "outputId": "30efc7cc-6c54-4a00-a101-c82178cdb1f9"
   },
   "outputs": [],
   "source": [
    "# Plot scores\n",
    "plot_model_scores(scores_inceptionnet, 'InceptionNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "nxTJnzkKm9sy",
    "outputId": "38260f8d-2501-437c-940b-6d4d56867b05"
   },
   "outputs": [],
   "source": [
    "# Print table results\n",
    "results = [('ResNet', runtime_resnet, scores_resnet)\n",
    "          ,('VGG-Net', runtime_vggnet, scores_vggnet)\n",
    "          ,('InceptionNet', runtime_inceptionnet, scores_inceptionnet)]\n",
    "table = []\n",
    "for i in range(len(results)):\n",
    "    tmp = {\n",
    "            'model': results[i][0]\n",
    "            , 'runtime (sec)': results[i][1]\n",
    "            , 'train_score (cat. accuracy)': results[i][2][0][-1]\n",
    "            , 'test_score (cat. accuracy)': results[i][2][1][-1]\n",
    "        }\n",
    "    table.append(tmp)\n",
    "\n",
    "\n",
    "pd.DataFrame(table).sort_values(by = ['test_score (cat. accuracy)'\n",
    "                                      ,'runtime (sec)']\n",
    "                                , ascending = [False\n",
    "                                               , True]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUC_1DUZnHx4"
   },
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "del results, tmp, table\n",
    "del get_model_results, plot_model_scores, get_compiled_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGVjHQmEnMi-"
   },
   "outputs": [],
   "source": [
    "# Load top model\n",
    "model = load_model('model_inceptionnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHHvcNNDnR1u"
   },
   "outputs": [],
   "source": [
    "# Create prefethed dataset of images to classify\n",
    "submit_data = image_dir + sample_data['image_id'] + '.jpg'\n",
    "submit_data = submit_data.values\n",
    "\n",
    "submit_dataset = get_prefetched_data((submit_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH9kwIeUnWoY",
    "outputId": "17c83820-0fc7-4292-cba0-b8ad284d9a22"
   },
   "outputs": [],
   "source": [
    "# Get results\n",
    "results = model.predict(submit_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki13wLFini7z"
   },
   "outputs": [],
   "source": [
    "# Merge results with sample submission\n",
    "sample_data.loc[:, 'healthy':] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Interface.__init__() missing 2 required positional arguments: 'inputs' and 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Exploratory Data\u001b[39;00m\n\u001b[0;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m---> 35\u001b[0m aks\u001b[38;5;241m=\u001b[39m\u001b[43mgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m,output \n\u001b[0;32m     37\u001b[0m df\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     39\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "\u001b[1;31mTypeError\u001b[0m: Interface.__init__() missing 2 required positional arguments: 'inputs' and 'outputs'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
